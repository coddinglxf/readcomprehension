非常好的总结性blog：
https://www.zybuluo.com/ShawnNg/note/622592

1：首先阅读理解任务中，一般句子较长，所以attention机制是比较重要的。一般来说，都是通过query去找document中最接近的部分，然后利用这部分的信息可以用来回答问题。这也符合人类思考问题的方式，读完题目，去文章中找答案，一般来说，答案都是集中在文章的某一个部分。

2：体现出信息迭代和不断学习的过程。这和人思考问题的方式是类似的，当我们去面对一个阅读理解问题后，读完文章和题目以后，我们很难第一时间就得出新的答案，那大部分时候，我们都是基于已有的信息，然后重新读题目和文章，在此基础上，我们可以得出新的一些结论，也许通过反复几次的循环，从而可以最终获得问题的答案。这种信息的迭代过程最直接的方式在于可以通过RNN这种结构来反映出来。通过将每一次编码出来的信息进行迭代，可以学习到最终有用的信息。

Stanford AR和The Attentive Reader
区别在于attention方式，还有就是预测的词表范围


以人的认知来说：分为两个部分进行:
1:读完文章和题目就直接开始做题，以后文章和题目也不看了。

2:反复的读文章和题目，不断地累积知识，所以问题的关键在于如何在每一次迭代时候选择有用的知识，所以此时attention的对象应该是存取的只是本身。
达到一定程度后，然后判断最终的答案。


区别：Attentive Reader和Impatient Reader
首先从计算效率上来看，Attentive Reader的效率要远高于Impatient Reader，这个事很明显的，因为Attentive Reader只需要在最终的隐含状态上做attention，而Impatient Reader需要在每个词上都要进行attention，但在实际效果上，Attentive Reader跟Impatient Reader在性能上是接近的。具体个单个情况下，大部分时候问题的句子是相对较短的，（<50个单词）通过选取合适的隐含节点数，LSTM完全是有可能将问题的信息编码好的，所以Attentive Reader模型的能力是可以得到保证的。所以从效率，效果，分析这几点上来看，个人认为Attentive Reader的性能是优于Impatient Reader的。但是Impatient Reader模型体现了一种迭代学习的概念


pointer network利用累积多个实体上面的概率之和。。。相当于显示的包含了数据的统计特性吧







